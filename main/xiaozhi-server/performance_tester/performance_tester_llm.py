import asyncio
import logging
import os
import statistics
import time
import concurrent.futures
from typing import Dict, Optional
import yaml
import aiohttp
from tabulate import tabulate
from core.utils.llm import create_instance as create_llm_instance
from config.settings import load_config

# Set the global log level to WARNING and suppress INFO level logs
logging.basicConfig(level=logging.WARNING)

description = "Large language model performance test"


class LLMPerformanceTester:
    def __init__(self):
        self.config = load_config()
        # Use test content that is more in line with the agent scenario, including system prompt words
        self.system_prompt = self._load_system_prompt()
        self.test_sentences = self.config.get("module_test", {}).get(
            "test_sentences",
            [
                "Hello, I'm not in a good mood today, can you comfort me?",
                "Could you please help me check what the weather will be like tomorrow?",
                "I want to hear an interesting story. Can you tell me one?",
                "What time is it now? What day is it today?",
                "I want to set an alarm for 8 o'clock tomorrow morning to remind me of a meeting",
            ],
        )
        self.results = {}

    def _load_system_prompt(self) -> str:
        """Load system prompt words"""
        try:
            prompt_file = os.path.join(
                os.path.dirname(os.path.dirname(__file__)), self.config.get("prompt_template", "agent-base-prompt.txt")
            )
            with open(prompt_file, "r", encoding="utf-8") as f:
                content = f.read()
                # Replace template variables with test values
                content = content.replace(
                    "{{base_prompt}}", "You are Xiaozhi, a smart and cute AI assistant"
                )
                content = content.replace(
                    "{{emojiList}}", "üòÄ,üòÉ,üòÑ,üòÅ,üòä,üòç,ü§î,üòÆ,üò±,üò¢,üò≠,üò¥,üòµ,ü§ó,üôÑ"
                )
                content = content.replace("{{current_time}}", "August 17, 2024 12:30:45")
                content = content.replace("{{today_date}}", "August 17, 2024")
                content = content.replace("{{today_weekday}}", "Saturday")
                content = content.replace("{{lunar_date}}", "July 14th, Year Jiachen")
                content = content.replace("{{local_address}}", "Beijing")
                content = content.replace("{{weather_info}}", "Sunny today, 25-32‚ÑÉ")
                return content
        except Exception as e:
            print(f"Unable to load system prompt word file: {e}")
            return "You are Xiaozhi, a smart and cute AI assistant. Please respond to users in a warm and friendly tone."

    def _collect_response_sync(self, llm, messages, llm_name, sentence_start):
        """Auxiliary method for synchronously collecting response data"""
        chunks = []
        first_token_received = False
        first_token_time = None

        try:
            response_generator = llm.response("perf_test", messages)
            chunk_count = 0
            for chunk in response_generator:
                chunk_count += 1
                # Every time a certain number of chunks are processed, check whether it should be interrupted.
                if chunk_count % 10 == 0:
                    # Exit early by checking if the current thread is marked as interrupted
                    import threading

                    if (
                        threading.current_thread().ident
                        != threading.main_thread().ident
                    ):
                        # If not the main thread, check if it should be stopped
                        pass

                # Check whether the chunk contains error information
                chunk_str = str(chunk)
                if (
                    "abnormal" in chunk_str
                    or "mistake" in chunk_str
                    or "502" in chunk_str.lower()
                ):
                    error_msg = chunk_str.lower()
                    print(f"{llm_name} response contains error message: {error_msg}")
                    # Throws an exception containing an error message
                    raise Exception(chunk_str)

                if not first_token_received and chunk.strip() != "":
                    first_token_time = time.time() - sentence_start
                    first_token_received = True
                    print(f"{llm_name} First Token: {first_token_time:.3f}s")
                chunks.append(chunk)
        except Exception as e:
            # More detailed error message
            error_msg = str(e).lower()
            print(f"{llm_name} response collection exception: {error_msg}")
            # For 502 errors or network errors, throw exceptions directly and let the upper layer handle them.
            if (
                "502" in error_msg
                or "bad gateway" in error_msg
                or "error code: 502" in error_msg
                or "abnormal" in str(e)
                or "mistake" in str(e)
            ):
                raise e
            # For other errors, partial results can be returned
            return chunks, first_token_time

        return chunks, first_token_time

    async def _check_ollama_service(self, base_url: str, model_name: str) -> bool:
        """Asynchronously checking Ollama service status"""
        async with aiohttp.ClientSession() as session:
            try:
                async with session.get(f"{base_url}/api/version") as response:
                    if response.status != 200:
                        print(f"Ollama service is not started or cannot be accessed: {base_url}")
                        return False
                async with session.get(f"{base_url}/api/tags") as response:
                    if response.status == 200:
                        data = await response.json()
                        models = data.get("models", [])
                        if not any(model["name"] == model_name for model in models):
                            print(
                                f"Ollama model {model_name} was not found, please use `ollama pull {model_name}` to download first"
                            )
                            return False
                    else:
                        print("Unable to get Ollama model list")
                        return False
                return True
            except Exception as e:
                print(f"Unable to connect to Ollama service: {str(e)}")
                return False

    async def _test_single_sentence(
        self, llm_name: str, llm, sentence: str
    ) -> Optional[Dict]:
        """Test performance on a single sentence"""
        try:
            print(f"{llm_name} Start test: {sentence[:20]}...")
            sentence_start = time.time()
            first_token_received = False
            first_token_time = None

            # Construct a message containing system prompt words
            messages = [
                {"role": "system", "content": self.system_prompt},
                {"role": "user", "content": sentence},
            ]

            # Use asyncio.wait_for for timeout control
            try:
                loop = asyncio.get_event_loop()
                with concurrent.futures.ThreadPoolExecutor() as executor:
                    # Create a response collection task
                    future = executor.submit(
                        self._collect_response_sync,
                        llm,
                        messages,
                        llm_name,
                        sentence_start,
                    )

                    # Use asyncio.wait_for to implement timeout control
                    try:
                        response_chunks, first_token_time = await asyncio.wait_for(
                            asyncio.wrap_future(future), timeout=10.0
                        )
                    except asyncio.TimeoutError:
                        print(f"{llm_name} test timeout (10 seconds), skip")
                        # Force cancel future
                        future.cancel()
                        # Wait a short period of time to ensure that the thread pool task can respond to cancellation
                        try:
                            await asyncio.wait_for(
                                asyncio.wrap_future(future), timeout=1.0
                            )
                        except (
                            asyncio.TimeoutError,
                            concurrent.futures.CancelledError,
                            Exception,
                        ):
                            # Ignore all exceptions and ensure program execution continues
                            pass
                        return None

            except Exception as timeout_error:
                print(f"{llm_name} handling exception: {timeout_error}")
                return None

            response_time = time.time() - sentence_start
            print(f"{llm_name} completed response: {response_time:.3f}s")

            return {
                "name": llm_name,
                "type": "llm",
                "first_token_time": first_token_time,
                "response_time": response_time,
            }
        except Exception as e:
            error_msg = str(e).lower()
            # Check if it is a 502 error or network error
            if (
                "502" in error_msg
                or "bad gateway" in error_msg
                or "error code: 502" in error_msg
            ):
                print(f"{llm_name} encountered a 502 error and skipped the test")
                return {
                    "name": llm_name,
                    "type": "llm",
                    "errors": 1,
                    "error_type": "502 network error",
                }
            print(f"{llm_name} sentence test failed: {str(e)}")
            return None

    async def _test_llm(self, llm_name: str, config: Dict) -> Dict:
        """Asynchronously test individual LLM performance"""
        try:
            # For Ollama, skip api_key check and do special handling
            if llm_name == "Ollama":
                base_url = config.get("base_url", "http://localhost:11434")
                model_name = config.get("model_name")
                if not model_name:
                    print("Ollama model_name not configured")
                    return {
                        "name": llm_name,
                        "type": "llm",
                        "errors": 1,
                        "error_type": "network error",
                    }

                if not await self._check_ollama_service(base_url, model_name):
                    return {
                        "name": llm_name,
                        "type": "llm",
                        "errors": 1,
                        "error_type": "network error",
                    }
            else:
                if "api_key" in config and any(
                    x in config["api_key"] for x in ["your", "placeholder", "sk-xxx"]
                ):
                    print(f"Skip unconfigured LLM: {llm_name}")
                    return {
                        "name": llm_name,
                        "type": "llm",
                        "errors": 1,
                        "error_type": "Configuration error",
                    }

            # Get the actual type (compatible with old configurations)
            module_type = config.get("type", llm_name)
            llm = create_llm_instance(module_type, config)

            # Uniformly use UTF-8 encoding
            test_sentences = [
                s.encode("utf-8").decode("utf-8") for s in self.test_sentences
            ]

            # Create a test task for all sentences
            sentence_tasks = []
            for sentence in test_sentences:
                sentence_tasks.append(
                    self._test_single_sentence(llm_name, llm, sentence)
                )

            # Execute all sentence tests concurrently and handle possible exceptions
            sentence_results = await asyncio.gather(
                *sentence_tasks, return_exceptions=True
            )

            # Process the results, filtering out exceptions and None values
            valid_results = []
            for result in sentence_results:
                if isinstance(result, dict) and result is not None:
                    valid_results.append(result)
                elif isinstance(result, Exception):
                    error_msg = str(result).lower()
                    if "502" in error_msg or "bad gateway" in error_msg:
                        print(f"{llm_name} encountered a 502 error, skip the sentence test")
                        return {
                            "name": llm_name,
                            "type": "llm",
                            "errors": 1,
                            "error_type": "502 network error",
                        }
                    else:
                        print(f"{llm_name} sentence test exception: {result}")

            if not valid_results:
                print(f"{llm_name} has no valid data. There may be network problems or configuration errors.")
                return {
                    "name": llm_name,
                    "type": "llm",
                    "errors": 1,
                    "error_type": "network error",
                }

            # Check the number of valid results and consider the test failed if too few
            if len(valid_results) < len(test_sentences) * 0.3:  # There must be at least a 30% success rate
                print(
                    f"{llm_name} has too few successful test sentences ({len(valid_results)}/{len(test_sentences)}). The network may be unstable or there may be a problem with the interface."
                )
                return {
                    "name": llm_name,
                    "type": "llm",
                    "errors": 1,
                    "error_type": "network error",
                }

            first_token_times = [
                r["first_token_time"]
                for r in valid_results
                if r.get("first_token_time")
            ]
            response_times = [r["response_time"] for r in valid_results]

            # Filter outlier data (data beyond 3 standard deviations)
            if len(response_times) > 1:
                mean = statistics.mean(response_times)
                stdev = statistics.stdev(response_times)
                filtered_times = [t for t in response_times if t <= mean + 3 * stdev]
            else:
                filtered_times = response_times

            return {
                "name": llm_name,
                "type": "llm",
                "avg_response": sum(response_times) / len(response_times),
                "avg_first_token": (
                    sum(first_token_times) / len(first_token_times)
                    if first_token_times
                    else 0
                ),
                "success_rate": f"{len(valid_results)}/{len(test_sentences)}",
                "errors": 0,
            }
        except Exception as e:
            error_msg = str(e).lower()
            if "502" in error_msg or "bad gateway" in error_msg:
                print(f"LLM {llm_name} encountered a 502 error and skipped the test")
            else:
                print(f"LLM {llm_name} test failed: {str(e)}")
            error_type = "network error"
            if "timeout" in str(e).lower():
                error_type = "Connection timed out"
            return {
                "name": llm_name,
                "type": "llm",
                "errors": 1,
                "error_type": error_type,
            }

    def _print_results(self):
        """Print test results"""
        print("\n" + "=" * 50)
        print("LLM performance test results")
        print("=" * 50)

        if not self.results:
            print("No test results available")
            return

        headers = ["Model name", "Average response time(s)", "First token time (s)", "success rate", "state"]
        table_data = []

        # Collect all data and categorize it
        valid_results = []
        error_results = []

        for name, data in self.results.items():
            if data["errors"] == 0:
                # normal result
                avg_response = f"{data['avg_response']:.3f}"
                avg_first_token = (
                    f"{data['avg_first_token']:.3f}"
                    if data["avg_first_token"] > 0
                    else "-"
                )
                success_rate = data.get("success_rate", "N/A")
                status = "‚úÖNormal"

                # Save the value used for sorting
                first_token_value = (
                    data["avg_first_token"]
                    if data["avg_first_token"] > 0
                    else float("inf")
                )

                valid_results.append(
                    {
                        "name": name,
                        "avg_response": avg_response,
                        "avg_first_token": avg_first_token,
                        "success_rate": success_rate,
                        "status": status,
                        "sort_key": first_token_value,
                    }
                )
            else:
                # Wrong result
                avg_response = "-"
                avg_first_token = "-"
                success_rate = "0/5"

                # Get specific error type
                error_type = data.get("error_type", "network error")
                status = f"‚ùå {error_type}"

                error_results.append(
                    [name, avg_response, avg_first_token, success_rate, status]
                )

        # Sort by first token time in ascending order
        valid_results.sort(key=lambda x: x["sort_key"])

        # Convert the sorted valid results into tabular data
        for result in valid_results:
            table_data.append(
                [
                    result["name"],
                    result["avg_response"],
                    result["avg_first_token"],
                    result["success_rate"],
                    result["status"],
                ]
            )

        # Add error results to the end of table data
        table_data.extend(error_results)

        print(tabulate(table_data, headers=headers, tablefmt="grid"))
        print("\nTest description:")
        print("- Test content: Agent dialogue scene containing complete system prompt words")
        print("- Timeout control: The maximum waiting time for a single request is 10 seconds")
        print("- Error handling: model that automatically skips 502 errors and network exceptions")
        print("- Success rate: number of sentences successfully responded/total number of test sentences")
        print("\nTest completed!")

    async def run(self):
        """Execute full asynchronous testing"""
        print("Start filtering available LLM modules...")

        # Create all test tasks
        all_tasks = []

        # LLM test tasks
        if self.config.get("LLM") is not None:
            for llm_name, config in self.config.get("LLM", {}).items():
                # Check configuration validity
                if llm_name == "CozeLLM":
                    if any(x in config.get("bot_id", "") for x in ["your"]) or any(
                        x in config.get("user_id", "") for x in ["your"]
                    ):
                        print(f"LLM {llm_name} not configured bot_id/user_id, skipped")
                        continue
                elif "api_key" in config and any(
                    x in config["api_key"] for x in ["your", "placeholder", "sk-xxx"]
                ):
                    print(f"LLM {llm_name} api_key not configured, skipped")
                    continue

                # For Ollama, first check the service status
                if llm_name == "Ollama":
                    base_url = config.get("base_url", "http://localhost:11434")
                    model_name = config.get("model_name")
                    if not model_name:
                        print("Ollama model_name not configured")
                        continue

                    if not await self._check_ollama_service(base_url, model_name):
                        continue

                print(f"Add LLM test task: {llm_name}")
                all_tasks.append(self._test_llm(llm_name, config))

        print(f"\n{len(all_tasks)} available LLM modules found")
        print("\nStart concurrent testing of all modules...\n")

        # Execute all test tasks concurrently but set independent timeouts for each task
        async def test_with_timeout(task, timeout=30):
            """Add timeout protection for each test task"""
            try:
                return await asyncio.wait_for(task, timeout=timeout)
            except asyncio.TimeoutError:
                print(f"Test task timeout ({timeout} seconds), skip")
                return {
                    "name": "Unknown",
                    "type": "llm",
                    "errors": 1,
                    "error_type": "Connection timed out",
                }
            except Exception as e:
                print(f"Test task exception: {str(e)}")
                return {
                    "name": "Unknown",
                    "type": "llm",
                    "errors": 1,
                    "error_type": "network error",
                }

        # Wrap timeout protection for each task
        protected_tasks = [test_with_timeout(task) for task in all_tasks]

        # Execute all test tasks concurrently
        all_results = await asyncio.gather(*protected_tasks, return_exceptions=True)

        # Processing results
        for result in all_results:
            if isinstance(result, dict):
                if result.get("errors") == 0:
                    self.results[result["name"]] = result
                else:
                    # Log even if there are errors, used to display failure status
                    if result.get("name") != "Unknown":
                        self.results[result["name"]] = result
            elif isinstance(result, Exception):
                print(f"Test result processing exception: {str(result)}")

        # Print results
        print("\nGenerate test report...")
        self._print_results()


async def main():
    tester = LLMPerformanceTester()
    await tester.run()


if __name__ == "__main__":
    asyncio.run(main())
